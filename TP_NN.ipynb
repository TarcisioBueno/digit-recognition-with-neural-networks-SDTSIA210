{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iafPdtuncbq7"
   },
   "source": [
    "# TP: MNIST with Neural Networks (NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OlKZ3Hnas7B4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow version 2.15.0\n",
      "Using keras version 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(\"Using tensorflow version \" + str(tf.__version__))\n",
    "print(\"Using keras version \" + str(keras.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_QLz9_jbRZq"
   },
   "source": [
    "## Loading and preparing the MNIST dataset\n",
    "Load the MNIST dataset made available by keras.datasets. Check the size of the training and testing sets. \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "gG83hGyVmijn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>(60000, 28, 28)</td>\n",
       "      <td>(60000,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>(10000, 28, 28)</td>\n",
       "      <td>(10000,)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Images    Labels\n",
       "Train  (60000, 28, 28)  (60000,)\n",
       "Test   (10000, 28, 28)  (10000,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The MNSIT dataset is ready to be imported from Keras into RAM\n",
    "# Warning: you cannot do that for larger databases (e.g., ImageNet)\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "# Size of the data \n",
    "df = pd.DataFrame([[train_images.shape, train_labels.shape], [test_images.shape, test_labels.shape]],\n",
    "                  columns=['Images', 'Labels'],\n",
    "                  index=['Train', 'Test'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRPbU_Z4U6Ac"
   },
   "source": [
    "The MNIST database contains 60,000 training images and 10,000 testing images.\n",
    "Using the pyplot package, visualize the first sample of the training set:\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VAu7oW0Zu4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us visualize the first training sample using the Matplotlib library with the imshow function\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.title('First sample of the training set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7YsRekMVDg-"
   },
   "source": [
    "The database contains images of handwritten digits. Hence, they belong to one of 10 categories, depending on the digit they represent. \n",
    "Reminder: in order to do multi-class classification, we use the softmax function, which outputs a multinomial probability distribution. That means that the output to our model will be a vector of size $10$, containing probabilities (meaning that the elements of the vector will be positive sum to $1$).\n",
    "For easy computation, we want to true labels to be represented with the same format: that is what we call **one-hot encoding**. For example, if an image $\\mathbf{x}$ represents the digit $5$, we have the corresponding one_hot label (careful, $0$ will be the first digit): \n",
    "$$ \\mathbf{y} = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0] $$\n",
    "Here, you need to turn train and test labels to one-hot encoding using the following function: \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQbkllF8mnaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_one_hot = to_categorical(train_labels)\n",
    "test_labels_one_hot = to_categorical(test_labels)\n",
    "\n",
    "# Checking the one=hot encoding\n",
    "label_number = 2\n",
    "print(train_labels[label_number])\n",
    "print(train_labels_one_hot[label_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jv29YLtVO3q"
   },
   "source": [
    "Images are black and white, with size $28 \\times 28$. We will work with them using a simple linear classification model, meaning that we will have them as vectors of size $(784)$.\n",
    "You should then transform the images to the size $(784)$ using the numpy function ```reshape```.\n",
    "\n",
    "Then, after casting the pixels to floats, normalize the images so that they have zero-mean and unitary deviation. Be careful to your methodology: while you have access to training data, you may not have access to testing data, and must avoid using any statistic on the testing dataset.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptTRSDo5nJyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshape images to vectors of pixels\n",
    "print(train_images.shape)\n",
    "\n",
    "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
    "train_images_transformed = train_images.reshape(train_images.shape[0], img_rows * img_cols)\n",
    "test_images_transformed = test_images.reshape(test_images.shape[0], img_rows * img_cols)\n",
    "\n",
    "\n",
    "# Cast pixels from uint8 to float32\n",
    "train_images_transformed = train_images_transformed.astype('float32')\n",
    "test_images_transformed = test_images_transformed.astype('float32')\n",
    "\n",
    "# Now let us normalize the images so that they have zero mean and standard deviation\n",
    "# Hint: are real testing data statistics known at training time ?\n",
    "\n",
    "train_images_transformed = (train_images_transformed - np.mean(train_images_transformed)) / np.std(train_images_transformed)\n",
    "test_images_transformed = (test_images_transformed - np.mean(test_images_transformed)) / np.std(test_images_transformed)\n",
    "\n",
    "print(train_images_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First part: working with Numpy\n",
    "\n",
    "Look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf) for some basic information on how to use numpy.\n",
    "\n",
    "### Defining the model \n",
    "\n",
    "We will here create a simple, linear classification model. We will take each pixel in the image as an input feature (making the size of the input to be $784$) and transform these features with a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$. Since there is $10$ possible classes, we want to obtain $10$ scores. Then, \n",
    "$$ \\mathbf{W} \\in \\mathbb{R}^{784 \\times 10} $$\n",
    "$$ \\mathbf{b} \\in \\mathbb{R}^{10} $$\n",
    "\n",
    "and our scores are obtained with:\n",
    "$$ \\mathbf{z} = \\mathbf{W}^{T} \\mathbf{x} +  \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathbb{R}^{784}$ is the input vector representing an image.\n",
    "We note $\\mathbf{y} \\in \\mathbb{R}^{10}$ as the target one_hot vector. \n",
    "\n",
    "Here, you fist need to initialize $\\mathbf{W}$ and $\\mathbf{b}$ using ```np.random.normal``` and ```np.zeros```, then compute $\\mathbf{z}$.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid implementing a complicated gradient back-propagation,\n",
    "# we will try a very simple architecture with one layer \n",
    "def initLayer(n_input,n_output):\n",
    "    \"\"\"\n",
    "    Initialize the weights, return the number of parameters\n",
    "    Inputs: n_input: the number of input units - int\n",
    "          : n_output: the number of output units - int\n",
    "    Outputs: W: a matrix of weights for the layer - numpy ndarray\n",
    "           : b: a vector bias for the layer - numpy ndarray\n",
    "           : nb_params: the number of parameters  - int\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Create W at the right size with a normal distribution\n",
    "    W = np.random.normal(size=(n_input, n_output))\n",
    "    # Create b at the right size, with zeros\n",
    "    b = np.zeros((n_output))\n",
    "    nb_params = np.prod(W.shape) + np.prod(b.shape)\n",
    "    return W, b, nb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = train_images.shape[0] \n",
    "n_feature = train_images.shape[1] * train_images.shape[2]\n",
    "n_labels = 10\n",
    "W, b, nb_params = initLayer(n_feature, n_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(W, b, X):\n",
    "    \"\"\"\n",
    "    Perform the forward propagation\n",
    "    Inputs: W: the weights - numpy ndarray\n",
    "          : b: the bias - numpy ndarray\n",
    "          : X: the batch - numpy ndarray\n",
    "    Outputs: z: outputs - numpy ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    z = np.dot(X, W) + b.T\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the output \n",
    "\n",
    "To obtain classification probabilities, we use the softmax function:\n",
    "$$ \\mathbf{o} = softmax(\\mathbf{z}) \\text{         with          } o_i = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)} $$\n",
    "\n",
    "The usual difficulty with the softmax function is the possibility of overflow when the scores $z_i$ are already large. Since a softmax is not affected by a shift affecting the whole vector $\\mathbf{z}$:\n",
    "$$ \\frac{\\exp(z_i - c)}{\\sum_{j=0}^{9} \\exp(z_j - c)} =  \\frac{\\exp(c) \\exp(z_i)}{\\exp(c) \\sum_{j=0}^{9} \\exp(z_j)} = \\frac{\\exp(z_i)}{\\sum_{j=0}^{9} \\exp(z_j)}$$\n",
    "what trick can we use to ensure we will not encounter any overflow ? \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Perform the softmax transformation to the pre-activation values\n",
    "    Inputs: z: the pre-activation values - numpy ndarray\n",
    "    Outputs: out: the activation values - numpy ndarray\n",
    "    \"\"\"\n",
    "    # Subtract the maximum value in z from all values in z\n",
    "    z_shifted = z - np.max(z, keepdims=True)\n",
    "\n",
    "    # Compute the exponential of the shifted input\n",
    "    exp_z = np.exp(z_shifted)\n",
    "\n",
    "    # Compute the softmax transformation\n",
    "    out = exp_z / np.sum(exp_z, keepdims=True)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making updates\n",
    "\n",
    "The update rule is affected by regularization. We implement two cases: No regularization, or L2 regularization. Use the two possible update rules to implement the following function: <div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(param, grad_param, eta, regularizer=None, weight_decay=0.):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: param: the network parameters - ndarray\n",
    "          : grad_param: the updates of the parameters - ndarray\n",
    "          : eta: the step-size of the gradient descent - float\n",
    "          : weight_decay: the weight-decay - float\n",
    "    Outputs: the parameters updated - ndarray\n",
    "    \"\"\"\n",
    "    if regularizer==None:\n",
    "        # Update rule without regularization\n",
    "        param -= eta * grad_param\n",
    "    elif regularizer=='L2':\n",
    "        # Update rule with L2 regularization\n",
    "        param -= eta * (grad_param + weight_decay * param)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a learning rate $\\eta$. The goal is to be able to apply updates:\n",
    "$$ \\mathbf{W}^{t+1} = \\mathbf{W}^{t} + \\nabla_{\\mathbf{W}} l_{MLE} $$\n",
    "\n",
    "In order to do this, we will compute this gradient (and the bias) in the function ```update```. In the next function ```updateParams```, we will actually apply the update with regularization. \n",
    "\n",
    "Reminder: the gradient $\\nabla_{\\mathbf{W}} l_{MLE}$ is the matrix containing the partial derivatives \n",
    "$$ \\left[\\frac{\\delta l_{MLE}}{\\delta W_{ij}}\\right]_{i=1..784, j=1..10} $$\n",
    "**Remark**: Careful, the usual way of implementing this in python has the dimensions of $\\mathbf{W}$ reversed compared to the notation of the slides.\n",
    "\n",
    "Coordinate by coordinate, we obtain the following update: \n",
    "$$ W_{ij}^{t+1} = W_{ij}^{t} + \\eta \\frac{\\delta l_{MLE}}{\\delta W_{ij}} $$\n",
    "\n",
    "Via the chain rule, we obtain, for an input feature $i \\in [0, 783]$ and a output class $j \\in [0, 9]$: $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = \\frac{\\delta l_{MLE}}{\\delta z_{j}} \\frac{\\delta z_j}{\\delta W_{ij}}$$ \n",
    "\n",
    "It's easy to compute that $\\frac{\\delta z_j}{\\delta W_{ij}} = x_i$\n",
    "\n",
    "We compute the softmax derivative, to obtain:\n",
    "$$ \\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y} $$\n",
    "\n",
    "Hence, $\\frac{\\delta l_{MLE}}{\\delta z_{j}} = o_j - y_j$ and we obtain that $$\\frac{\\delta l_{MLE}}{\\delta W_{ij}} = (o_j - y_j) x_i$$\n",
    "\n",
    "This can easily be written as a scalar product, and a similar computation (even easier, actually) can be done for $\\mathbf{b}$. Noting $\\nabla_{\\mathbf{z}} l_{MLE} = \\mathbf{o} - \\mathbf{y}$ as ```grad``` in the following function, compute the gradients $\\nabla_{\\mathbf{W}} l_{MLE}$ and $\\nabla_{\\mathbf{b}} l_{MLE}$ in order to call the function ```updateParams```.\n",
    "\n",
    "Note: the regularizer and the weight_decay $\\lambda$ are used in ```updateParams```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(eta, W, b, grad, X, regularizer, weight_decay):\n",
    "    \"\"\"\n",
    "    Perform the update of the parameters\n",
    "    Inputs: eta: the step-size of the gradient descent - float \n",
    "          : W: the weights - ndarray\n",
    "          : b: the bias -  ndarray\n",
    "          : grad: the gradient of the activations w.r.t. to the loss -  list of ndarray\n",
    "          : X: the data -  ndarray\n",
    "          : regularizer: 'L2' or None - the regularizer to be used in updateParams\n",
    "          : weight_decay: the weight decay to be used in updateParams - float\n",
    "    Outputs: W: the weights updated -  ndarray\n",
    "           : b: the bias updated -  ndarray\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_w = np.outer(X, grad)\n",
    "    grad_b = np.sum(grad, axis=0)\n",
    "    \n",
    "    W = updateParams(W, grad_w, eta, regularizer, weight_decay)\n",
    "    b = updateParams(b, grad_b, eta, regularizer, weight_decay)\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Accuracy\n",
    "\n",
    "Here, we simply use the model to predict the class (by taking the argmax of the output !) for every example in ```X```, and count the number of times the model is right, to output the accuracy.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAcc(W, b, X, labels):\n",
    "    \"\"\"\n",
    "    Compute the loss value of the current network on the full batch\n",
    "    Inputs: act_func: the activation function - function\n",
    "          : W: the weights - list of ndarray\n",
    "          : B: the bias - list of ndarray\n",
    "          : X: the batch - ndarray\n",
    "          : labels: the labels corresponding to the batch\n",
    "    Outputs: loss: the negative log-likelihood - float\n",
    "           : accuracy: the ratio of examples that are well-classified - float\n",
    "    \"\"\" \n",
    "    # Forward propagation\n",
    "    z = forward(W, b, X)\n",
    " \n",
    "    # Compute the softmax and the prediction\n",
    "    out = softmax(z)\n",
    "    pred = np.argmax(out, axis=1)\n",
    "    \n",
    "    #Â Compute the accuracy\n",
    "    accuracy = np.mean(pred == labels)\n",
    "\n",
    "      \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training\n",
    "\n",
    "The following hyperparameters are given. Next, we can assemble all the function previously defined to implement a training loop. We will train the classifier on **one epoch**, meaning that the model will see each training example once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "eta = 0.01\n",
    "regularizer = 'L2'\n",
    "weight_decay = 0.0001\n",
    "\n",
    "# Training\n",
    "log_interval = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.05806666666666667 0.0595 0.01\n",
      "5000 0.8168 0.8249 0.01\n",
      "10000 0.8271666666666667 0.8379 0.01\n",
      "15000 0.8344666666666667 0.8461 0.01\n",
      "20000 0.85955 0.8644 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 0.8498 0.8521 0.01\n",
      "30000 0.8644 0.8692 0.01\n",
      "35000 0.8666333333333334 0.8634 0.01\n",
      "40000 0.8790333333333333 0.8761 0.01\n",
      "45000 0.86715 0.8651 0.01\n",
      "50000 0.88295 0.8863 0.01\n",
      "55000 0.86785 0.8652 0.01\n",
      "Final result: 0.86785 0.8652 0.01\n"
     ]
    }
   ],
   "source": [
    "# Data structures for plotting\n",
    "g_train_acc=[]\n",
    "g_valid_acc=[]\n",
    "\n",
    "#######################\n",
    "### Learning process ##\n",
    "#######################\n",
    "for j in range(n_training):\n",
    "    # Getting the example\n",
    "    X, y = train_images_transformed[j], train_labels_one_hot[j]\n",
    "\n",
    "    # Forward propagation\n",
    "    z = forward(W, b, X)\n",
    "\n",
    "    # Compute the softmax\n",
    "    out = softmax(z)\n",
    "        \n",
    "    # Compute the gradient at the top layer\n",
    "    derror = out - y # This is o - y \n",
    "\n",
    "    # Update the parameters\n",
    "    W, b = update(eta, W, b, derror, X, regularizer, weight_decay)\n",
    "\n",
    "    if j % log_interval == 0:\n",
    "        # Every log_interval examples, look at the training accuracy\n",
    "        train_accuracy = computeAcc(W, b, train_images_transformed, train_labels) \n",
    "\n",
    "        # And the testing accuracy\n",
    "        test_accuracy = computeAcc(W, b, test_images_transformed, test_labels) \n",
    "\n",
    "        g_train_acc.append(train_accuracy)\n",
    "        g_valid_acc.append(test_accuracy)\n",
    "        result_line = str(int(j)) + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "        print(result_line)\n",
    "\n",
    "g_train_acc.append(train_accuracy)\n",
    "g_valid_acc.append(test_accuracy)\n",
    "result_line = \"Final result:\" + \" \" + str(train_accuracy) + \" \" + str(test_accuracy) + \" \" + str(eta)\n",
    "print(result_line)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the performance of this simple linear classifier ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part: Autoencoder with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder and PCA\n",
    "\n",
    "First, we will try to connect the representation produced by Principal Component Analysis with what is learnt by a simple, linear, autoencoder. We will use the ```scikit-learn``` implementation of the ```PCA``` to obtain the two first components (hint: use the attribute ```.components_```), and visualize them:\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'n_components' parameter of PCA must be an int in the range [0, inf), a float in the range (0.0, 1.0), a str among {'mle'} or None. Got Ellipsis instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Let's find the first 2 PCA components\u001b[39;00m\n\u001b[0;32m      4\u001b[0m num_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 5\u001b[0m pca \u001b[38;5;241m=\u001b[39m \u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Reshape so they resemble images and we can print them\u001b[39;00m\n\u001b[0;32m      8\u001b[0m eigen_mnist \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mcomponents_\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1145\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1141\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1142\u001b[0m )\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1145\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:638\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    631\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 638\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py:96\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     )\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'n_components' parameter of PCA must be an int in the range [0, inf), a float in the range (0.0, 1.0), a str among {'mle'} or None. Got Ellipsis instead."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Let's find the first 2 PCA components\n",
    "num_components = 2\n",
    "pca = PCA(...).fit(...)\n",
    "\n",
    "# Reshape so they resemble images and we can print them\n",
    "eigen_mnist = pca.components_.reshape(...)\n",
    "\n",
    "# Show the reshaped principal components\n",
    "f, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(eigen_mnist[0], cmap='gray')\n",
    "ax[0].set_xlabel('First Principal Component')\n",
    "ax[1].imshow(eigen_mnist[1], cmap='gray')\n",
    "ax[1].set_xlabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variance explained by those components\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the visualization in relation to the variance explained by only keeping the two principal components:\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Autoencoder with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will use Keras to implement the autoencoder. You can take a look at this [cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf) for some basic commands to use keras.\n",
    "\n",
    "In this first case, we implement a **simple linear autoencoder**. Build it in order to have the same capacity as the PCA decomposition (2 hidden dimensions !) we made just above. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_layer = ...\n",
    "\n",
    "# Encoding layer\n",
    "latent_view = ...\n",
    "\n",
    "# Decoding layer\n",
    "output_layer = ...\n",
    "\n",
    "ae_model = Model(input_layer, output_layer, name='ae_model')\n",
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What loss shoud we use ? Choose the usual one and import it directly from Keras. You can use a simple ```SGD``` optimizer, and then compile the model; finally, train it to rebuild images from the original examples. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import ...\n",
    "loss = ...\n",
    "\n",
    "optimizer = SGD(lr=1e-1) \n",
    "ae_model.compile(optimizer=optimizer, loss=loss) \n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "# No noise here - we want to train a simple auto-encoder and compare visually with PCA\n",
    "history = ae_model.fit(...,\n",
    "                       ...,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=1,\n",
    "                       shuffle=True,\n",
    "                       validation_data=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the name of your layer (obtained through the command ```model.summary()```) is ```'layer'```, here is the way to obtained the weights. Visualize the weights of the encoder and compare them to the two components obtained through the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = ae_model.get_layer('layer').get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the two dimensions of the encoder, in a similar manner to the principal components\n",
    "# (after reshaping them as images !)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, visualize the images rebuilt by the network !\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few images at random: look from n\n",
    "n = np.random.randint(0,len(test_images)-5)\n",
    "\n",
    "#Â Plot a few images from n  \n",
    "f, ax = plt.subplots(1,5)\n",
    "for i,a in enumerate(range(n,n+5)):\n",
    "    ...\n",
    "    \n",
    "# Get the prediction from the model \n",
    "\n",
    "\n",
    "# ... and plot them \n",
    "f, ax = plt.subplots(1,5)\n",
    "for i,a in enumerate(range(n,n+5)):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same ( = build a new model) with a latent dimension that is largely higher than 2. Compare the visualizations and the images that are rebuilt. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: De-noising Autoencoder\n",
    "\n",
    "Now, we can implement a **de-noising autoencoder**. The following function will transform an array of images by adding it random noise. Create a new autoencoder model, this time with **more layers** and **non-linear activations** (like the ReLU) and train it to rebuild the de-noised images. Display some testing images, with noise, and re-built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(array):\n",
    "    \"\"\"\n",
    "    Adds random noise to each image in the supplied array.\n",
    "    \"\"\"\n",
    "    noise_factor = 0.4\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "    return noisy_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data with added noise\n",
    "noisy_train_images = noise(train_images)\n",
    "noisy_test_images = noise(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the images with noise against the originals\n",
    "\n",
    "\n",
    "# Build a new model with more layers and Relu activations\n",
    "\n",
    "\n",
    "# Compile it but here, use noised data as inputs !\n",
    "\n",
    "\n",
    "# Visualize the images rebuilt by the model !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we normalize the images to be in the 0-1 range, what other loss function could we use ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Answer:</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TP4_1_empty.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
